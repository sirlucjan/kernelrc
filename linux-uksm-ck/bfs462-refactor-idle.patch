From 400e6fd1172dd77cf73c43a4ca798fbfc5956f41 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 12 Nov 2014 00:09:32 +0800
Subject: [PATCH] bfs: Refactor idle!=prev and next task block in schedule().

Just refactory the code and doesn't change the logic. Reduce a dupicated
condition sentence and some not operations.
---
 kernel/sched/bfs.c | 56 ++++++++++++++++++++++--------------------------------
 1 file changed, 23 insertions(+), 33 deletions(-)

diff --git a/kernel/sched/bfs.c b/kernel/sched/bfs.c
index 31ddb7c..f585dc1 100644
--- a/kernel/sched/bfs.c
+++ b/kernel/sched/bfs.c
@@ -1215,20 +1215,6 @@ static inline void take_task(int cpu, struct task_struct *p)
 	dec_qnr();
 }
 
-/*
- * Returns a descheduling task to the grq runqueue unless it is being
- * deactivated.
- */
-static inline void return_task(struct task_struct *p, struct rq *rq, bool deactivate)
-{
-	if (deactivate)
-		deactivate_task(p, rq);
-	else {
-		inc_qnr();
-		enqueue_task(p, rq);
-	}
-}
-
 /* Enter with grq lock held. We know p is on the local cpu */
 static inline void __set_tsk_resched(struct task_struct *p)
 {
@@ -3517,28 +3503,38 @@ static void __sched __schedule(void)
 		check_deadline(prev);
 		prev->last_ran = rq->clock_task;
 
-		/* Task changed affinity off this CPU */
-		if (likely(!needs_other_cpu(prev, cpu))) {
-			if (!deactivate) {
-				if (!queued_notrunning()) {
+		if (deactivate)
+			deactivate_task(prev, rq);
+		else {
+			/* Task changed affinity off this CPU */
+			if (likely(!needs_other_cpu(prev, cpu))) {
+				if (queued_notrunning())
+					swap_sticky(rq, cpu, prev);
+				else {
 					/*
-					 * We now know prev is the only thing that is
-					 * awaiting CPU so we can bypass rechecking for
-					 * the earliest deadline task and just run it
-					 * again.
-					 */
+					* We now know prev is the only thing that is
+					* awaiting CPU so we can bypass rechecking for
+					* the earliest deadline task and just run it
+					* again.
+					*/
 					set_rq_task(rq, prev);
 					check_smt_siblings(cpu);
 					grq_unlock_irq();
 					goto rerun_prev_unlocked;
-				} else
-					swap_sticky(rq, cpu, prev);
+				}
 			}
+			inc_qnr();
+			enqueue_task(prev, rq);
 		}
-		return_task(prev, rq, deactivate);
 	}
 
-	if (unlikely(!queued_notrunning())) {
+	if (likely(queued_notrunning())) {
+		next = earliest_deadline_task(rq, cpu, idle);
+		if (likely(next->prio != PRIO_LIMIT))
+			clear_cpuidle_map(cpu);
+		else
+			set_cpuidle_map(cpu);
+	} else {
 		/*
 		 * This CPU is now truly idle as opposed to when idle is
 		 * scheduled as a high priority task in its own right.
@@ -3546,12 +3542,6 @@ static void __sched __schedule(void)
 		next = idle;
 		schedstat_inc(rq, sched_goidle);
 		set_cpuidle_map(cpu);
-	} else {
-		next = earliest_deadline_task(rq, cpu, idle);
-		if (likely(next->prio != PRIO_LIMIT))
-			clear_cpuidle_map(cpu);
-		else
-			set_cpuidle_map(cpu);
 	}
 
 	if (likely(prev != next)) {
-- 
2.1.1
 
